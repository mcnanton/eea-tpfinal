# SHAP values for model interpretability. An application to genomic data modeling.

## Description

This small project demonstrates the application of SHAP (SHapley Additive exPlanations) for understanding the performance of a non-interpretable algorithm (lightGBM) applied to genomic data modeling. 



## Slides (in spanish): 
[Google Slides](https://docs.google.com/presentation/d/1JAB7PZxTdLXwQ24uVTAUsQwy5jU-cWM1EYicAmeZxgs/edit?usp=sharing)

## Videos (in spanish):
[Youtube](https://www.youtube.com/playlist?list=PLN2e9R_DoC0Suz5gnEs1xM4iITsgCrepb)

## Resources: 
* Book [Cristoph Molner - “Interpretable Machine learning”](https://christophm.github.io/interpretable-ml-book/shapley.html)
* Article [“A new perspective on Shapley values, part I: Intro to Shapley and SHAP”](https://edden-gerber.github.io/shapley-part-1/)
* Article [“Kernel SHAP, un paso adelante”](https://blogs.sas.com/content/sasla/2021/04/26/kernel-shap-un-paso-adelante-serie-explicate/)
* [Paper Lloyd Shapley](https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM670.pdf)
* Paper [Scott Lundberg y Su-In Lee SHAP](https://arxiv.org/abs/1705.07874)
* Paper [Scott Lundberg y Su-In Lee TreeSHAP](https://arxiv.org/abs/1802.03888)
* Video [“GTO-7-03: The Shapley Value”](https://www.youtube.com/watch?v=qcLZMYPdpH4)
* [Interpreting machine learning models to investigate circadian regulation and facilitate exploration of clock function](https://www.pnas.org/doi/10.1073/pnas.2103070118)
* [Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI](https://www.sciencedirect.com/science/article/abs/pii/S1566253519308103?via%3Dihub)
* [Molecular Biology of the Cell](https://www.ncbi.nlm.nih.gov/books/NBK21054/)
* Article [“Understanding Shapley value explanation algorithms for trees”](https://hughchen.github.io/its_blog/index.html)
* [Derecho a la explicación](https://en.wikipedia.org/wiki/Right_to_explanation)
* [Ley de igualdad de Oportunidad de Crédito (EE.UU.)](https://www.ecfr.gov/current/title-12/chapter-X/part-1002/section-1002.9)
* [Explainable machine-learning predictions for the prevention of hypoxaemia during surgery](https://www.nature.com/articles/s41551-018-0304-0.epdf?author_access_token=vSPt7ryUfdSCv4qcyeEuCdRgN0jAjWel9jnR3ZoTv0PdqacSN9qNY_fC0jWkIQUd0L2zaj3bbIQEdrTqCczGWv2brU5rTJPxyss1N4yTIHpnSv5_nBVJoUbvejyvvjrGTb2odwWKT2Bfvl0ExQKhZw%3D%3D)
* Curso de [Explainable AI por Sun-In Lee (desarrolladora de SHAP)](https://sites.google.com/cs.washington.edu/csep590b?pli=1)
* Article [“Problems with Shapley-value-based explanations as feature importance measures”](https://www.researchgate.net/publication/339497849_Problems_with_Shapley-value-based_explanations_as_feature_importance_measures)
* Article [“Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods”](https://dl.acm.org/doi/abs/10.1145/3375627.3375830)
